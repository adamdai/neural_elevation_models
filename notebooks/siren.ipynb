{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "from plotly.subplots import make_subplots\n",
    "import tinycudann as tcnn\n",
    "import torch.nn as nn\n",
    "\n",
    "from nemo.nemo import Nemo\n",
    "from nemo.util import grid_2d\n",
    "from nemo.siren import Siren\n",
    "from nemo.plotting import plot_surface, plot_path_3d\n",
    "from nemo.dynamics import diff_flatness_siren\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get data to fit\n",
    "nemo = Nemo()\n",
    "nemo.load_weights('../models/AirSimMountains/AirSimMountains_encs.pth', '../models/AirSimMountains/AirSimMountains_mlp.pth')\n",
    "\n",
    "N = 512\n",
    "bounds = np.array([-0.75, 0.45, -0.6, 0.6])\n",
    "xy, grid = grid_2d(N, bounds)\n",
    "xy = xy.to(device)\n",
    "z = nemo.get_heights(xy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SIREN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "siren = Siren(in_features=2, out_features=1, hidden_features=256,\n",
    "                hidden_layers=3, outermost_linear=True).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train\n",
    "\n",
    "Siren weights are torch.float32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next(siren.named_parameters())[1].dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss function\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# Optimizer\n",
    "optimizer = torch.optim.Adam(siren.parameters(), lr=1e-5)\n",
    "\n",
    "# Convert data from half to float\n",
    "xy = xy.to(torch.float32).to(device)\n",
    "z = z.to(torch.float32).to(device)\n",
    "\n",
    "# Train the network\n",
    "for step in range(1000):\n",
    "    # Forward pass\n",
    "    pred = siren(xy)\n",
    "\n",
    "    # Compute loss\n",
    "    loss = criterion(pred, z)\n",
    "\n",
    "    # Backward pass\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward(retain_graph=True)\n",
    "    optimizer.step()\n",
    "\n",
    "    # Print loss every 500 steps\n",
    "    if step % 500 == 0:\n",
    "        print(f\"Step {step}, Loss {loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample the Siren network to get the predicted elevation\n",
    "with torch.no_grad():\n",
    "    pred = siren(xy)\n",
    "\n",
    "# Plot the predictions\n",
    "z_grid = pred.reshape(N, N).detach().cpu().numpy()\n",
    "x_grid = grid[:,:,0].detach().cpu().numpy()\n",
    "y_grid = grid[:,:,1].detach().cpu().numpy()\n",
    "\n",
    "fig = plot_surface(x_grid, y_grid, z_grid, no_axes=False, showscale=False)\n",
    "fig.update_layout(width=1200, height=700, scene_aspectmode='data')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(siren.state_dict(), '../models/airsim_mountains_siren.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "siren.load_state_dict(torch.load('../models/airsim_mountains_siren.pt'))\n",
    "\n",
    "N = 512\n",
    "bounds = np.array([-0.75, 0.45, -0.6, 0.6])\n",
    "xy, grid = grid_2d(N, bounds)\n",
    "xy = xy.to(device)\n",
    "z = siren(xy)\n",
    "\n",
    "x_grid = grid[:,:,0].detach().cpu().numpy()\n",
    "y_grid = grid[:,:,1].detach().cpu().numpy()\n",
    "z_grid = z.reshape(N, N).detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plot_surface(x_grid, y_grid, z_grid, no_axes=False, showscale=False)\n",
    "fig.update_layout(width=1200, height=700, scene_aspectmode='data')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot gradients\n",
    "_, grad = siren.forward_with_grad(xy.clone().requires_grad_(True))\n",
    "\n",
    "grad = grad.reshape(N, N, 2).detach().cpu().numpy()\n",
    "grad_x = grad[:,:,0]\n",
    "grad_y = grad[:,:,1]\n",
    "\n",
    "fig = make_subplots(rows=1, cols=2, subplot_titles=('X Gradient', 'Y Gradient'), horizontal_spacing=0.15)\n",
    "fig.add_trace(go.Heatmap(z=grad_x, colorbar=dict(len=1.05, x=0.44, y=0.5)), row=1, col=1)\n",
    "fig.add_trace(go.Heatmap(z=grad_y, colorbar=dict(len=1.05, x=1.01, y=0.5)), row=1, col=2)\n",
    "fig.update_layout(width=1300, height=600, scene_aspectmode='data')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Path planning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nemo.global_planner import AStarGradPlanner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "nemo_start = np.array([0.404, 0.156, -0.613])\n",
    "nemo_end = np.array([-0.252, -0.228, -0.477])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the planner with scaled heightmap\n",
    "astar_heights = 1e4 * (z_grid + 1.0).reshape(N, N)\n",
    "gp = AStarGradPlanner(astar_heights, bounds)\n",
    "\n",
    "# Start and end positions for path\n",
    "start = tuple(nemo_start[:2])\n",
    "end = tuple(nemo_end[:2])\n",
    "\n",
    "# Compute path\n",
    "path_xy = gp.spatial_plan(start, end)\n",
    "path_xy_torch = torch.tensor(path_xy, dtype=torch.float32, device=device)\n",
    "# Get heights along path\n",
    "path_zs = siren(path_xy_torch)  \n",
    "\n",
    "# Save path as torch tensor\n",
    "astar_path = torch.cat((path_xy_torch, path_zs), dim=1)\n",
    "\n",
    "init_xy = path_xy_torch\n",
    "init_xy = init_xy.to(torch.float32).to(device)\n",
    "\n",
    "init_z = siren(init_xy)\n",
    "init_path = torch.cat((init_xy, init_z), dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure()\n",
    "fig = plot_surface(x_grid, y_grid, z_grid, fig=fig, no_axes=True)\n",
    "fig = plot_path_3d(x=init_path[:,0].detach().cpu().numpy(),\n",
    "                        y=init_path[:,1].detach().cpu().numpy(),\n",
    "                        z=init_path[:,2].detach().cpu().numpy(), color='red', fig=fig)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_start = init_xy[0]\n",
    "path_end = init_xy[-1]\n",
    "path_opt = init_xy[1:-1].clone().detach().requires_grad_(True)  # portion of the path to optimize\n",
    "path = torch.cat((path_start[None], path_opt, path_end[None]), dim=0)  # full path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "u = diff_flatness_siren(path, siren, dt=1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test gradient flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = 1.0\n",
    "xy = path\n",
    "\n",
    "x = xy[:, 0]\n",
    "y = xy[:, 1]\n",
    "epsilon = torch.tensor(1e-5, device=device, requires_grad=True)\n",
    "xdot = torch.hstack((epsilon, torch.diff(x) / dt))\n",
    "ydot = torch.hstack((epsilon, torch.diff(y) / dt))\n",
    "xddot = torch.hstack((epsilon, torch.diff(xdot) / dt))\n",
    "yddot = torch.hstack((epsilon, torch.diff(ydot) / dt))\n",
    "v = torch.sqrt(xdot**2 + ydot**2)\n",
    "theta = torch.arctan2(ydot, xdot)\n",
    "\n",
    "_, grad = siren.forward_with_grad(xy.clone().requires_grad_(True))\n",
    "psi = torch.atan2(grad[:,1], grad[:,0])\n",
    "alpha = torch.atan(grad.norm(dim=1))\n",
    "\n",
    "phi = alpha * torch.cos(theta - psi)\n",
    "g_eff = 9.81 * torch.sin(phi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cvxpy as cp\n",
    "from cvxpylayers.torch import CvxpyLayer\n",
    "\n",
    "u = cp.Variable(2)\n",
    "J = cp.Parameter((2, 2))\n",
    "b = cp.Parameter(2)\n",
    "objective = cp.Minimize(cp.norm(J @ u - b))\n",
    "problem = cp.Problem(objective)\n",
    "assert problem.is_dpp()\n",
    "\n",
    "cvxpylayer = CvxpyLayer(problem, parameters=[J, b], variables=[u])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "J_tch = torch.tensor([[torch.cos(theta[i]), -v[i] * torch.sin(theta[i])],\n",
    "                        [torch.sin(theta[i]), v[i] * torch.cos(theta[i])]], device=device, requires_grad=True)\n",
    "b_tch = torch.tensor([xddot[i] + g_eff[i] * torch.cos(theta[i]),\n",
    "                        yddot[i] + g_eff[i] * torch.sin(theta[i])], device=device, requires_grad=True)\n",
    "\n",
    "solution, = cvxpylayer(J_tch, b_tch)\n",
    "\n",
    "solution.sum().backward()\n",
    "solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "u = torch.zeros(len(x), 2)\n",
    "for i in range(len(x)):\n",
    "    J_tch = torch.stack([torch.cos(theta[i]), -v[i] * torch.sin(theta[i]),\n",
    "                     torch.sin(theta[i]), v[i] * torch.cos(theta[i])]).reshape(2, 2)\n",
    "    b_tch = torch.stack([xddot[i] + g_eff[i] * torch.cos(theta[i]),\n",
    "                     yddot[i] + g_eff[i] * torch.sin(theta[i])])\n",
    "    u[i], = cvxpylayer(J_tch, b_tch)\n",
    "\n",
    "    # c_test = torch.sum(u[i])\n",
    "    # c_test.backward(retain_graph=True)  # Use retain_graph to avoid re-building\n",
    "    # print(path_opt.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = torch.sum(u**2)\n",
    "c.backward()\n",
    "print(path_opt.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Path optimization\n",
    "\n",
    "TODO: \n",
    "* in addition to penalizing control input, also penalize change in control inputs\n",
    "* enforce control input limits\n",
    "* fix initial heading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_xy = init_xy\n",
    "path_start = path_xy[0]\n",
    "path_end = path_xy[-1]\n",
    "path_opt = path_xy[1:-1].clone().detach().requires_grad_(True)  # portion of the path to optimize\n",
    "\n",
    "# Optimize path\n",
    "opt = torch.optim.Adam([path_opt], lr=1e-3)\n",
    "\n",
    "a_cost_coeff = 1.0\n",
    "omega_cost_coeff = 1000.0\n",
    "\n",
    "for it in range(100):\n",
    "    opt.zero_grad()\n",
    "    path = torch.cat((path_start[None], path_opt, path_end[None]), dim=0)\n",
    "    u = diff_flatness_siren(path, siren, dt=1.0)\n",
    "\n",
    "    c = a_cost_coeff * torch.sum(u[:,0]**2) + omega_cost_coeff * torch.sum(u[:,1]**2)\n",
    "    \n",
    "    c.backward()\n",
    "    opt.step()\n",
    "    if it % 10 == 0:\n",
    "        print(f'it: {it},  Cost: {c.item()}')\n",
    "\n",
    "print(f'Finished optimization - final cost: {c.item()}')\n",
    "\n",
    "# Compute final heights and get full 3D path\n",
    "path_zs = siren(path)\n",
    "path_3d = torch.cat((path, path_zs), dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = u[:,0]\n",
    "omega = u[:,1]\n",
    "\n",
    "# Plot over time\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=np.arange(len(a)), y=a.detach().cpu().numpy(), mode='lines', name='a'))\n",
    "fig.add_trace(go.Scatter(x=np.arange(len(omega)), y=omega.detach().cpu().numpy(), mode='lines', name='omega'))\n",
    "fig.update_layout(title='Control inputs over time', xaxis_title='Time', yaxis_title='Control Input')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure()\n",
    "fig = plot_surface(x_grid, y_grid, z_grid, fig=fig, no_axes=True)\n",
    "fig = plot_path_3d(x=path_3d[:,0].detach().cpu().numpy(), \n",
    "                        y=path_3d[:,1].detach().cpu().numpy(), \n",
    "                        z=path_3d[:,2].detach().cpu().numpy(), color='orange', fig=fig)\n",
    "fig = plot_path_3d(x=init_path[:,0].detach().cpu().numpy(),\n",
    "                        y=init_path[:,1].detach().cpu().numpy(),\n",
    "                        z=init_path[:,2].detach().cpu().numpy(), color='red', fig=fig)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cooper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cooper\n",
    "from nemo.dynamics import forward_dynamics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forward dynamics\n",
    "\n",
    "# x, y, theta, v\n",
    "x_start = torch.tensor([0.404, 0.156], device=device)\n",
    "theta_start = torch.tensor(0.0, device=device)\n",
    "v_start = torch.tensor(0.0, device=device)\n",
    "x_goal = torch.tensor([-0.252, -0.228], device=device)\n",
    "v_goal = torch.tensor(0.0, device=device)\n",
    "# goal theta is unconstrained\n",
    "\n",
    "# Initial control inputs\n",
    "num_steps = 100\n",
    "u = torch.zeros(num_steps, 2, device=device)\n",
    "init_state = torch.tensor([0.404, 0.156, 0.0, 0.0], device=device)\n",
    "\n",
    "traj = forward_dynamics(init_state, u, siren, dt=1.0)\n",
    "\n",
    "resulting_path_2d = traj[:,:2].to(device)\n",
    "resulting_path_z = siren(resulting_path_2d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure()\n",
    "fig = plot_surface(x_grid, y_grid, z_grid, fig=fig, no_axes=True)\n",
    "fig = plot_path_3d(x=resulting_path_2d[:,0].detach().cpu().numpy(), \n",
    "                        y=resulting_path_2d[:,1].detach().cpu().numpy(), \n",
    "                        z=resulting_path_z.detach().cpu().numpy(), color='orange', fig=fig)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaximumEntropy(cooper.ConstrainedMinimizationProblem):\n",
    "    def __init__(self, mean_constraint):\n",
    "        self.mean_constraint = mean_constraint\n",
    "        super().__init__(is_constrained=True)\n",
    "\n",
    "    def closure(self, probs):\n",
    "        # Verify domain of definition of the functions\n",
    "        assert torch.all(probs >= 0)\n",
    "\n",
    "        # Negative signed removed since we want to *maximize* the entropy\n",
    "        entropy = torch.sum(probs * torch.log(probs))\n",
    "\n",
    "        # Entries of p >= 0 (equiv. -p <= 0)\n",
    "        ineq_defect = -probs\n",
    "\n",
    "        # Equality constraints for proper normalization and mean constraint\n",
    "        mean = torch.sum(torch.tensor(range(1, len(probs) + 1)) * probs)\n",
    "        eq_defect = torch.stack([torch.sum(probs) - 1, mean - self.mean_constraint])\n",
    "\n",
    "        return cooper.CMPState(loss=entropy, eq_defect=eq_defect, ineq_defect=ineq_defect)\n",
    "\n",
    "# Define the problem and formulation\n",
    "cmp = MaximumEntropy(mean_constraint=4.5)\n",
    "formulation = cooper.LagrangianFormulation(cmp)\n",
    "\n",
    "# Define the primal parameters and optimizer\n",
    "probs = torch.nn.Parameter(torch.rand(6)) # Use a 6-sided die\n",
    "primal_optimizer = cooper.optim.ExtraSGD([probs], lr=3e-2, momentum=0.7)\n",
    "\n",
    "# Define the dual optimizer. Note that this optimizer has NOT been fully instantiated\n",
    "# yet. Cooper takes care of this, once it has initialized the formulation state.\n",
    "dual_optimizer = cooper.optim.partial_optimizer(cooper.optim.ExtraSGD, lr=9e-3, momentum=0.7)\n",
    "\n",
    "# Wrap the formulation and both optimizers inside a ConstrainedOptimizer\n",
    "coop = cooper.ConstrainedOptimizer(formulation, primal_optimizer, dual_optimizer)\n",
    "\n",
    "# Here is the actual training loop.\n",
    "# The steps follow closely the `loss -> backward -> step` Pytorch workflow.\n",
    "for iter_num in range(5000):\n",
    "    coop.zero_grad()\n",
    "    lagrangian = formulation.composite_objective(cmp.closure, probs)\n",
    "    formulation.custom_backward(lagrangian)\n",
    "    coop.step(cmp.closure, probs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nemo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
